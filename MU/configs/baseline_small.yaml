# Baseline Transformer Configuration

model:
  type: "baseline_transformer"
  vocab_size: 50257  # GPT-2 tokenizer vocab size
  d_model: 128
  n_layers: 6
  n_heads: 4
  d_ff: 512
  dropout: 0.1
  max_seq_len: 256

training:
  batch_size: 32
  num_epochs: 10
  learning_rate: 0.0003
  weight_decay: 0.01
  warmup_steps: 500
  gradient_clip: 1.0
  mixed_precision: true

  # Loss weights (no invariance loss for baseline)
  lambda_lm: 1.0
  lambda_inv: 0.0

  # Optimizer
  optimizer: "adamw"
  betas: [0.9, 0.999]
  eps: 1.0e-8

  # Scheduler
  scheduler: "cosine_warmup"
  min_lr_ratio: 0.0

  # Logging
  log_interval: 100
  eval_interval: 1000
  save_interval: 1000
  max_checkpoints: 5

data:
  dataset: "wikitext-2"
  sequence_length: 256
  num_workers: 4
  vocab_size: 50257
