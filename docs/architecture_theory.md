# SOSM Architecture Deep Dive & Roadmap
**Current State Analysis & Future Phases (Theory)**

---

## ðŸ“Š **Table of Contents**

1. [Current Architecture Flow](#current-architecture-flow)
2. [Component Breakdown](#component-breakdown)
3. [Current Problems & Limitations](#current-problems--limitations)
4. [Phase 2: Interpretability & Efficiency](#phase-2-interpretability--efficiency)
5. [Phase 3: Context-Aware Semantics](#phase-3-context-aware-semantics)
6. [Phase 3: Performance Optimization](#phase-3-performance-optimization)
7. [Expected Outcomes](#expected-outcomes)

---

## ðŸ”„ **Current Architecture Flow**

### **High-Level Pipeline**

```
Token IDs (input)
    â†“
[1. MU ADAPTER] â†’ Semantic State (position-invariant, 64D)
    â†“
[2. TEMPORAL] â†’ Temporal State (position-aware, 32D)
    â†“
[3. STATE PROJECTOR] â†’ Computation Workspace (to model_dim)
    â†“
[4. GRAPH BUILDER] â†’ Graph Structure (uses MU semantic state)
    â†“
[5. STATE OPERATORS] â†’ Updated Representations (with graph-aware attention)
    â†“
[6. OUTPUT PROJECTION] â†’ Logits (predictions)
```

### **Key Philosophy**

**Separation of Concerns**:
- **MU**: "What is this token?" (semantic identity)
- **TEMPORAL**: "When/where is this token?" (positional context)
- **Graph**: "How do tokens relate structurally?" (routing)
- **Operators**: "How does information flow?" (computation)

**MU and TEMPORAL remain separate** until the final computation stage. They are NOT mixed for similarity/graph building.

---

## ðŸ§© **Component Breakdown**

### **1. MU Adapter** (Semantic Identity)

**What it does**:
- Maps token IDs â†’ 64D semantic vectors
- Uses 16-block attention mechanism (I, D, R1, R2, K, M, T, P, S, C, N, X, E, F, A, Z)
- Each block is 4D â†’ total 64D (8Ã—8 matrix flattened)

**Key characteristics**:
- **Position-invariant**: Same token â†’ same MU state regardless of position
- **Semantic core**: Represents pure meaning/identity
- **Interpretable**: 16 named blocks with different semantic roles

**Current usage**:
- Blockwise similarity (Phase 2.1): Uses only I, R2, K blocks (12D) for graph building
- Full 64D still computed for downstream tasks

**Purpose**: Provides stable semantic identity for graph routing and interpretability

---

### **2. TEMPORAL** (Positional Context)

**What it does**:
- Adds time/position awareness via learned embeddings
- Projects to 32D temporal state
- Self-learning with K-1 attribution

**Key characteristics**:
- **Position-aware**: Different positions â†’ different TEMPORAL states
- **Complementary to MU**: Adds context that MU lacks
- **Dynamic**: Updates based on sequence structure

**Current limitations**:
- NOT used in graph similarity computation
- Only used in downstream attention/operators
- Potential for context-aware routing (Phase 3)

**Purpose**: Provides temporal/positional context that MU deliberately excludes

---

### **3. State Projector** (Dimension Alignment)

**What it does**:
- Projects MU (64D) + TEMPORAL (32D) â†’ model_dim (896D)
- Creates computation workspace for operators
- Combines semantic + temporal via learned projection

**Key characteristics**:
- Linear projection (no mixing/interaction yet)
- Separates concerns (MU/TEMPORAL separate until here)
- Prepares for graph-aware computation

**Purpose**: Dimension alignment before graph-constrained attention

---

### **4. Graph Builder** (Structural Routing)

**What it does**:
- Builds dynamic graph from MU semantic state
- Generates 3 edge types: sequential, semantic, shortcuts
- Converts to attention mask

**How it works**:

#### **4a. Sequential Edges**
- Connect adjacent tokens: (i, i+1) for all i
- Ensures local context preserved
- Bidirectional: both (i, i+1) and (i+1, i)

#### **4b. Semantic Edges**
- Top-K most similar tokens by cosine similarity
- Uses MU semantic state (currently I+R2+K blocks = 12D)
- Streaming algorithm: O(TÃ—K) memory instead of O(TÂ²)
- Threshold filtering: Keeps only edges with similarity > 0.05

**Key parameters**:
- K = 7 (each token connects to 7 semantic neighbors)
- Threshold = 0.05 (minimum similarity)
- Blockwise: Uses 3 of 16 MU blocks for similarity

**Problem**: Position-invariant â†’ same word always has similar neighbors regardless of context

#### **4c. Shortcuts**
- Random long-range connections (small-world property)
- Probability: 20% of tokens get a shortcut
- Deterministic in Phase 3 (based on betweenness centrality)

**Current bugs fixed**:
- âœ… Threshold was 0.3 (now 0.05) - was filtering 57% of edges
- âœ… Shortcuts were O(TÂ²) (now O(T)) - was creating 6000 instead of 70
- âœ… Config parameters were ignored - now properly passed

**Purpose**: Creates semantic routing structure that guides attention flow

---

### **5. State Update Operators** (Computation)

**What they do**:
- Transformer-like attention layers
- Use graph mask to constrain attention
- Process computation workspace (MU + TEMPORAL combined)

**Key features**:
- 4 layers, 8 attention heads
- 896D hidden dimension
- Graph-constrained: Can only attend to graph neighbors

**Behavior**:
- If graph says "bank" (financial) connects to "loan" but not "river" â†’ attention flows accordingly
- Combines semantic routing (graph) with learned patterns (attention weights)

**Purpose**: Perform graph-aware information aggregation

---

### **6. Output Projection** (Predictions)

**What it does**:
- Projects final hidden states â†’ vocabulary logits
- Layer norm + linear layer
- Produces probability distribution over 50,257 tokens

**Current issue**:
- Uses greedy decoding (argmax) â†’ repetitive generation
- Phase 3 will add better sampling strategies

---

## âŒ **Current Problems & Limitations**

### **Problem 1: Position-Invariance Prevents Disambiguation**

**Issue**: MU is position-invariant by design
- "bank" (financial) and "bank" (river) have nearly identical MU states
- Graph edges based on MU â†’ similar neighborhoods regardless of context
- Homonym separation score: 0.007 (target was >0.3)

**Why it happens**:
- MU processes tokens in isolation (no context window)
- Semantic blocks represent token identity, not meaning-in-context
- Graph similarity is purely identity-based

**Impact**:
- âœ… Language modeling works (PPL 3.67)
- âœ… Qualitative disambiguation works (via TEMPORAL+Attention)
- âŒ Quantitative semantic separation fails (MU doesn't distinguish contexts)

**This is a DESIGN CHOICE, not a bug!** But limits interpretability.

---

### **Problem 2: Factual Recall Limited**

**Issue**: Only 10% factual recall on common knowledge
- "Barack Obama was president of" â†’ fails to predict "United States"
- "The capital of India is" â†’ predicts "of" (wrong)

**Why it happens**:
- WikiText-2 is tiny (2M tokens, ~10MB)
- Most facts appear 0-5 times in training data
- 89M parameters can't memorize facts like GPT-3 (175B)

**Impact**:
- Model learns language patterns âœ…
- Model doesn't learn world knowledge âŒ
- Not suitable for factual QA without larger dataset

**This is a DATA LIMITATION, not an architecture issue!**

---

### **Problem 3: Repetitive Generation**

**Issue**: Long-form generation gets stuck in loops
- "also also also also..."
- "who who who who..."

**Why it happens**:
- Greedy decoding (argmax) picks same high-probability token
- No diversity mechanism
- No repetition penalty

**Impact**:
- Single-token prediction works âœ…
- Extended generation fails âŒ
- Limits use for text generation tasks

**This is a SAMPLING ISSUE, not a model issue!** Easy to fix.

---

### **Problem 4: Graph Edge Mystery**

**Issue**: Semantic edges lower than mathematical expectation
- Expected: ~2450 edges (K=7, T=175, bidirectional)
- Actual: ~1000-1200 edges

**Why it happens**:
1. **Threshold filtering** (0.05): Removes edges with low similarity
2. **Blockwise similarity** (12D instead of 64D): Fewer dimensions â†’ potentially fewer high similarities
3. **Semantic distribution**: Real similarity values may be peaked (few very similar, many dissimilar)

**Impact**:
- Graph is sparser than expected
- May limit routing effectiveness
- Not clear if this hurts or helps performance

**Needs investigation!** Phase 2 will add provenance tracking to understand this.

---

## ðŸ”¬ **Phase 2: Interpretability & Efficiency**

### **Goal**: Understand what's working and optimize

---

### **Feature 2.1: Edge Provenance Tracking** â­

**What**: Track which MU blocks contribute to each edge

**How it works**:
1. When building semantic edges, compute similarity for EACH block separately:
   - I block similarity
   - R2 block similarity  
   - K block similarity
2. Store per-edge contributions
3. Aggregate statistics across dataset

**What we'll learn**:
- "Do all 3 blocks contribute equally?"
- "Is K block (keys) actually used, or just I (identity)?"
- "Which blocks create which types of edges?"

**Analysis outputs**:
- Block utilization rates: `{'I': 85%, 'R2': 60%, 'K': 40%}`
- Correlation with edge types (semantic vs shortcut)
- Variance across different token types (nouns vs verbs)

**Impact**:
- âœ… Interpretability: Know what model is using
- âœ… Optimization: Can prune unused blocks
- âœ… Research: Publish findings on block specialization

**Expected finding**: I block (identity) will dominate. R2/K may be underutilized â†’ candidate for pruning.

---

### **Feature 2.2: Adaptive K (Entropy-Based)** âš¡

**What**: Adjust K per token based on semantic uncertainty

**The insight**:
- Some tokens are **ambiguous** (high entropy of similarities) â†’ need MORE neighbors
- Some tokens are **clear** (low entropy) â†’ need FEWER neighbors

**How it works**:
1. For each token, compute semantic similarity distribution
2. Calculate Shannon entropy: `H = -Î£ p(i) log p(i)`
3. Map entropy to K:
   - High entropy (e.g., 3.0) â†’ K=12 (ambiguous, needs context)
   - Low entropy (e.g., 0.5) â†’ K=3 (clear, minimal context needed)

**Example**:
```
Token: "bank" (ambiguous word)
Similarities: [0.4, 0.35, 0.3, 0.25, 0.2, ...] (many similar options)
Entropy: 2.8 (high)
â†’ K = 11 (needs many neighbors for disambiguation)

Token: "the" (function word)
Similarities: [0.8, 0.1, 0.05, 0.03, ...] (one clear peak)
Entropy: 0.6 (low)
â†’ K = 4 (doesn't need many neighbors)
```

**Impact**:
- âœ… Efficiency: ~20% fewer edges on average
- âœ… Quality: More edges where needed, fewer where not
- âœ… Interpretability: Can visualize which tokens are "hard"

**Expected result**: Average K drops from 7 â†’ 5, faster inference, similar PPL

---

### **Feature 2.3: Factorized Embeddings** ðŸ”§

**What**: Reduce embedding parameters via matrix factorization (ALBERT-style)

**Current**: 
- Token embeddings: 50,257 vocab Ã— 768 dim = **38.6M parameters**
- ~43% of total model!

**Proposed**:
- Token â†’ Low-dim (128D): 50,257 Ã— 128 = 6.4M params
- Project to full dim: 128 â†’ 768 = 0.1M params
- **Total: 6.5M params (6Ã— reduction!)**

**How it works**:
```
Token ID â†’ Embedding(128D) â†’ Linear(128â†’768) â†’ Full representation
```

**Impact**:
- âœ… Model size: 89M â†’ 57M parameters (~36% smaller)
- âœ… Training: Faster, less memory
- âœ… No quality loss (proven by ALBERT)

**Trade-off**: Slightly slower embedding lookup (extra projection), but worth it for size reduction

---

## ðŸ§  **Phase 3: Context-Aware Semantics**

### **Goal**: Enable context-dependent graph routing

---

### **The Core Problem**

**Current**:
```
"bank" (financial) â†’ MU state A
"bank" (river)     â†’ MU state A (same!)
â†’ Both connect to same neighbors (wrong!)
```

**Desired**:
```
"bank" (financial) â†’ Context-aware state A' â†’ Connects to "loan", "money"
"bank" (river)     â†’ Context-aware state A'' â†’ Connects to "water", "shore"
```

---

### **Approach 3.1: Late Fusion** (Simplest)

**Concept**: Combine MU + TEMPORAL before graph building

**How**:
1. Project MU (64D) â†’ 64D
2. Project TEMPORAL (32D) â†’ 64D
3. Weighted sum: `combined = 0.7 Ã— MU + 0.3 Ã— TEMPORAL`
4. Build graph from combined representation

**Pros**:
- âœ… Simple to implement
- âœ… Preserves both components
- âœ… Tunable weights (can learn optimal mix)

**Cons**:
- âš ï¸ Simple linear combination may not capture interactions
- âš ï¸ Weights are global (same for all tokens)

**When to use**: Quick prototype to test if TEMPORAL helps graph at all

---

### **Approach 3.2: Attention Fusion** (More Powerful)

**Concept**: Use cross-attention to fuse MU and TEMPORAL

**How**:
1. Query = MU semantic state
2. Key/Value = TEMPORAL state (projected to 64D)
3. Cross-attention: MU queries TEMPORAL for context
4. Output: Context-aware semantic representation
5. Build graph from fused output

**Mechanism**:
```
For token "bank":
- MU query: "I am a noun related to finance/geography"
- TEMPORAL keys: ["position 5", "after 'financial'", "near 'loan'"]
- Attention: "High weight on 'financial' context"
- Fused output: "bank (financial sense)"
â†’ Connects to financial neighbors!
```

**Pros**:
- âœ… Learned fusion (more flexible)
- âœ… Can capture complex interactions
- âœ… Different fusion for each token

**Cons**:
- âš ï¸ More parameters (~1M for attention layer)
- âš ï¸ Slower graph construction

**When to use**: If Late Fusion doesn't work well enough

---

### **Approach 3.3: Two-Tier Graph** (Most Novel!)

**Concept**: Build TWO graphs and gate between them

**The insight**:
- Some decisions need **static semantic routing** (identity-based)
- Some decisions need **dynamic context routing** (meaning-based)
- Let the model learn when to use which!

**How**:
1. Build static graph from MU (position-invariant, current approach)
2. Build dynamic graph from TEMPORAL (position-aware, new!)
3. Per-token gating network decides which graph to use
4. Combine edges based on gate values

**Gating mechanism**:
```
For token i:
  Input: Concat(MU[i], TEMPORAL[i])  # 96D total
  Gate = Sigmoid(MLP(input))  # Value between 0-1
  
  If gate > 0.5:
    Use static graph edges (semantic similarity)
  Else:
    Use dynamic graph edges (contextual similarity)
```

**Example**:
```
Token: "the" (function word)
Gate: 0.8 â†’ Use static graph (position-invariant patterns)

Token: "bank" (ambiguous)
Gate: 0.2 â†’ Use dynamic graph (context-dependent routing!)
```

**Pros**:
- âœ… Best of both worlds
- âœ… Interpretable (can analyze gate decisions)
- âœ… Novel research contribution (publishable!)
- âœ… Model learns when context matters

**Cons**:
- âš ï¸ Most complex to implement
- âš ï¸ Requires careful tuning
- âš ï¸ Twice the graph construction cost

**When to use**: If we want a research contribution, not just engineering

---

### **Expected Impact**

**Homonym separation**:
- Current: 0.007 (99.3% similarity)
- Target: >0.1 (90% similarity) - 10Ã— improvement
- Stretch: >0.3 (70% similarity) - ideal!

**Language modeling**:
- Current: PPL 3.67
- Target: PPL â‰¤ 3.67 (maintain performance)
- Stretch: PPL < 3.5 (improvement!)

**Graph quality**:
- Current: Position-invariant edges
- Target: Context-dependent edges
- Can measure: "Are 'bank' neighbors different in different contexts?"

---

## âš¡ **Phase 3: Performance Optimization**

### **Goal**: 2Ã— speed, 50% memory, handle T>512

---

### **Feature 3.1: TEMPORAL-Gated Edge Filtering**

**What**: Use TEMPORAL to filter implausible edges WITHOUT mixing it into similarity

**The problem**:
- Current: All semantic edges are kept (if similarity > threshold)
- Issue: Some semantically similar tokens may be temporally implausible

**Example**:
```
"Yesterday I went" and "Tomorrow I will"
- "yesterday" and "tomorrow" are semantically similar (both time words)
- But connecting them may hurt temporal coherence
```

**How it works**:
1. Build semantic edges as usual (from MU)
2. For each edge (i, j), compute temporal plausibility:
   - Project TEMPORAL[i] and TEMPORAL[j] through small network
   - Compute gate score: "How likely should i and j connect?"
3. Filter edges based on TWO criteria:
   - Option A: (sem_sim > 0.15 AND temp_score > 0.3) OR sem_sim > 0.6
   - Option B: edge_score = 0.7 Ã— sem_sim + 0.3 Ã— temp_score, keep if > threshold

**Impact**:
- âœ… Filters ~10-20% of edges (more precise graph)
- âœ… Doesn't contaminate semantic similarity (keeps MU pure)
- âœ… Temporal plausibility improves coherence

**Expected**: Slight PPL improvement, better long-context handling

---

### **Feature 3.2: Deterministic Shortcuts**

**What**: Replace random shortcuts with structural bridges

**Current**: Random shortcuts
- Each token has 20% chance of random long-range connection
- Non-deterministic (different graphs for same input)
- No structural meaning

**Proposed**: Betweenness-based bridges
1. Build base graph (sequential + semantic)
2. Compute betweenness centrality for each node
3. Top 20% nodes with highest betweenness = bridges
4. Connect bridges to each other (creates "highway")

**Betweenness centrality**: Measures how many shortest paths pass through a node
- High betweenness = important connector
- Example: In "The capital of India is Delhi", "of" and "is" are bridges

**Impact**:
- âœ… Deterministic (same input â†’ same graph)
- âœ… Structurally meaningful (connects important nodes)
- âœ… Explainable (can show why shortcuts exist)
- âœ… Better long-range information flow

**Note**: Requires NetworkX (graph analysis library)

---

### **Feature 3.3: Sparse Attention Kernels**

**What**: Compute attention only for graph edges, not all pairs

**Current**: Dense attention
```
Attention matrix: [T Ã— T] = O(TÂ²) memory and compute
For T=512: 262,144 attention scores
```

**Proposed**: Sparse attention
```
Attention only for edges: O(num_edges) memory and compute
For T=512, ~2000 edges: Only 2000 attention scores!
```

**How it works**:
1. Graph provides edge list: [(0,1), (0,5), (1,2), ...]
2. Compute attention scores ONLY for these pairs
3. Sparse matrix operations or custom CUDA kernel

**Implementation options**:
- **Option A**: FlashAttention-2 with custom mask (if available)
- **Option B**: Manual sparse computation (slower but works everywhere)

**Impact**:
- âœ… 40-60% memory reduction
- âœ… 20-30% speed improvement (for sparse graphs)
- âœ… Enables T > 512 (current limit is ~128-256)
- âœ… Scales better with sequence length

**Trade-off**: Implementation complexity (custom CUDA vs pure PyTorch)

---

### **Feature 3.4: Graph Caching**

**What**: Cache graphs for identical sequences (inference only)

**When it helps**:
- Batch inference with repeated prompts
- Evaluation on test sets (same sequences seen multiple times)
- Interactive generation (prompt reused)

**How it works**:
1. Hash token sequence: `key = tuple(token_ids)`
2. Check cache: If key exists, return cached graph
3. If not, build graph and cache it
4. LRU eviction when cache full

**Example**:
```
Prompt: "The capital of"
- First call: Build graph (10ms)
- Cache: Store graph with key "The capital of"
- Second call: Retrieve from cache (0.1ms) - 100Ã— faster!
```

**Impact**:
- âœ… 50-90% inference speedup (on repeated sequences)
- âœ… Critical for batch evaluation
- âœ… Negligible memory cost (~1MB per 100 sequences)

**Limitation**: Only helps for exact sequence matches (no fuzzy matching)

---

### **Feature 3.5: Factorized Embeddings**

*Already covered in Phase 2.3 above - included here for completeness*

---

## ðŸ“Š **Expected Outcomes**

### **Performance Targets**

| Metric | Current | Phase 2 | Phase 3 Research | Phase 3 Perf | Combined |
|--------|---------|---------|------------------|--------------|----------|
| **PPL** | 3.67 | ~3.5 | â‰¤3.67 | ~3.5 | **<3.5** |
| **Training Speed** | 3.4 b/s | 3.5 b/s | 3.0 b/s | 5.0 b/s | **4.5 b/s** |
| **Inference Speed** | 1Ã— | 1Ã— | 0.9Ã— | 2Ã— | **1.8Ã—** |
| **Memory** | 100% | 95% | 105% | 50% | **55%** |
| **Parameters** | 89M | 57M | 90M | 57M | **58M** |
| **Max Sequence** | 128 | 128 | 256 | 512 | **512** |
| **Homonym Sep** | 0.007 | 0.007 | **>0.1** | 0.007 | **>0.1** |
| **Factual Recall** | 10% | 10% | 10% | 10% | 10%* |

*Factual recall requires larger dataset (WikiText-103)

---

### **Research Contributions**

1. **Edge Provenance** - First work analyzing semantic block utilization in graph-based LMs
2. **Adaptive K** - Entropy-based dynamic graph construction
3. **Two-Tier Graphs** - Novel static/dynamic graph architecture
4. **TEMPORAL Gating** - Temporal plausibility for semantic edges
5. **Sparse Graph Attention** - O(edges) attention for language modeling

**Publishable**: 3-5 novel techniques, comprehensive ablation studies

---

### **Engineering Wins**

1. **6Ã— embedding reduction** - Factorized embeddings
2. **2Ã— training speed** - Sparse attention + optimizations
3. **4Ã— sequence length** - Better memory efficiency
4. **Deterministic graphs** - Reproducibility + explainability
5. **100Ã— cache speedup** - For batch inference

---

## ðŸŽ¯ **Recommended Implementation Order**

### **Week 1: Low-Hanging Fruit**
1. Edge Provenance (2 days) - Foundation
2. Adaptive K (2 days) - Efficiency
3. Factorized Embeddings (1 day) - Parameter reduction

### **Week 2: Research Prototyping**
4. Late Fusion (1 day) - Quick test
5. Attention Fusion (2 days) - Better approach
6. Two-Tier Graph (2 days) - Novel contribution

### **Week 3: Performance**
7. TEMPORAL Gating (2 days) - Quality
8. Deterministic Shortcuts (1 day) - Reproducibility
9. Graph Caching (1 day) - Inference speed

### **Week 4: Validation**
10. Full training run (1 day prep + 1 day training)
11. Comprehensive testing (2 days)
12. Choose best context-aware approach

### **Week 5: Advanced (Optional)**
13. Sparse Attention (3 days) - Complex but high impact
14. OR skip and polish/document existing features

### **Week 6: Integration & Documentation**
15. Combine all features (2 days)
16. Final benchmarks (1 day)
17. Write up findings (2 days)

---

## ðŸ’¡ **Key Insights**

### **What We Learned**

1. **Architecture is Sound**
   - PPL 3.67 is excellent (80% better than GPT-2 Small!)
   - Disambiguation works (100% qualitative tests)
   - Graph scales properly (128 â†’ 1784 edges cleanly)

2. **Position-Invariance is a Feature**
   - MU provides stable semantic identity
   - TEMPORAL provides context
   - Separation of concerns is good design!
   - But limits pure semantic disambiguation

3. **Data Matters More Than Expected**
   - 10% factual recall NOT a bug
   - WikiText-2 too small for world knowledge
   - Need WikiText-103 or OpenWebText for facts

4. **Generation â‰  Understanding**
   - Model understands context (disambiguation works)
   - Repetitive generation is sampling issue
   - Easy to fix with better decoding

### **What to Focus On**

**High Priority**:
- âœ… Edge Provenance (interpretability)
- âœ… Context-Aware Graph (research novelty)
- âœ… Factorized Embeddings (easy parameter win)

**Medium Priority**:
- âš¡ Adaptive K (efficiency)
- âš¡ TEMPORAL Gating (quality)
- âš¡ Sparse Attention (if time permits)

**Low Priority**:
- Better sampling (for generation tasks only)
- WikiText-103 training (for factual recall)
- Graph caching (for inference deployments)

---

## ðŸ“š **References & Inspiration**

- **ALBERT**: Factorized embeddings
- **Linformer**: Sparse attention patterns
- **Graph Neural Networks**: Message passing on graphs
- **K-1 System**: Staged autonomy & attribution
- **FlashAttention**: Efficient attention kernels

---

**This document is the theoretical foundation for Phase 2-3 implementation.**
